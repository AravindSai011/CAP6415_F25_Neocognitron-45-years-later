# -*- coding: utf-8 -*-
"""models/necognitron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15DteNj8lVQaEbpcD9c1lAwvdZm56Z0zs
"""

# src/models/neocognitron.py

"""
Neocognitron-inspired CNN model.

This module implements a simple convolutional network that mimics the
S/C-layer idea of the Neocognitron:

- S-layers: feature extraction via convolution + nonlinearity
- C-layers: pooling for position tolerance

The model is designed for MNIST (1x28x28 grayscale digits).
"""

from typing import Tuple

import torch
import torch.nn as nn


class Neocognitron(nn.Module):
    """
    Neocognitron-like convolutional neural network.

    Architecture (for MNIST 1x28x28):
        - S1: Conv2d(1 -> 32, kernel 5, padding 2) + ReLU
        - C1: MaxPool2d(2x2)
        - S2: Conv2d(32 -> 64, kernel 5, padding 2) + ReLU
        - C2: MaxPool2d(2x2)
        - FC: Flatten -> Linear -> ReLU -> Linear(num_classes)

    Args:
        input_channels: Number of channels in the input image (1 for MNIST).
        num_classes: Number of output classes (10 for digits 0â€“9).
    """

    def __init__(self, input_channels: int = 1, num_classes: int = 10):
        super().__init__()

        # First S-layer: local feature extraction
        self.s1 = nn.Conv2d(
            in_channels=input_channels,
            out_channels=32,
            kernel_size=5,
            padding=2,
        )

        # First C-layer: pooling for translation tolerance
        self.c1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Second S-layer: higher-level features
        self.s2 = nn.Conv2d(
            in_channels=32,
            out_channels=64,
            kernel_size=5,
            padding=2,
        )

        # Second C-layer: pooling
        self.c2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # After two 2x2 pools on 28x28 -> 7x7 feature maps
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the Neocognitron-like model.

        Args:
            x: Input batch tensor of shape [B, C, H, W].

        Returns:
            logits: Output tensor of shape [B, num_classes] (unnormalized).
        """
        # S1 + ReLU
        x = torch.relu(self.s1(x))
        # C1 pooling
        x = self.c1(x)

        # S2 + ReLU
        x = torch.relu(self.s2(x))
        # C2 pooling
        x = self.c2(x)

        # Classifier head
        logits = self.classifier(x)
        return logits

    def num_parameters(self) -> int:
        """Return total number of trainable parameters (for logging/debugging)."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

